{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaypatel/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from textblob import TextBlob\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jaypatel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class InferSent(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(InferSent, self).__init__()\n",
    "        self.bsize = config['bsize']\n",
    "        self.word_emb_dim = config['word_emb_dim']\n",
    "        self.enc_lstm_dim = config['enc_lstm_dim']\n",
    "        self.pool_type = config['pool_type']\n",
    "        self.dpout_model = config['dpout_model']\n",
    "        self.version = 1 if 'version' not in config else config['version']\n",
    "\n",
    "        self.enc_lstm = nn.LSTM(self.word_emb_dim, self.enc_lstm_dim, 1,\n",
    "                                bidirectional=True, dropout=self.dpout_model)\n",
    "\n",
    "        assert self.version in [1, 2]\n",
    "        if self.version == 1:\n",
    "            self.bos = '<s>'\n",
    "            self.eos = '</s>'\n",
    "            self.max_pad = True\n",
    "            self.moses_tok = False\n",
    "        elif self.version == 2:\n",
    "            self.bos = '<p>'\n",
    "            self.eos = '</p>'\n",
    "            self.max_pad = False\n",
    "            self.moses_tok = True\n",
    "\n",
    "    def is_cuda(self):\n",
    "        # either all weights are on cpu or they are on gpu\n",
    "        return self.enc_lstm.bias_hh_l0.data.is_cuda\n",
    "\n",
    "    def forward(self, sent_tuple):\n",
    "        # sent_len: [max_len, ..., min_len] (bsize)\n",
    "        # sent: (seqlen x bsize x worddim)\n",
    "        sent, sent_len = sent_tuple\n",
    "\n",
    "        # Sort by length (keep idx)\n",
    "        sent_len_sorted, idx_sort = np.sort(sent_len)[::-1], np.argsort(-sent_len)\n",
    "        sent_len_sorted = sent_len_sorted.copy()\n",
    "        idx_unsort = np.argsort(idx_sort)\n",
    "\n",
    "        idx_sort = torch.from_numpy(idx_sort).cuda() if self.is_cuda() \\\n",
    "            else torch.from_numpy(idx_sort)\n",
    "        sent = sent.index_select(1, idx_sort)\n",
    "\n",
    "        # Handling padding in Recurrent Networks\n",
    "        sent_packed = nn.utils.rnn.pack_padded_sequence(sent, sent_len_sorted)\n",
    "        sent_output = self.enc_lstm(sent_packed)[0]  # seqlen x batch x 2*nhid\n",
    "        sent_output = nn.utils.rnn.pad_packed_sequence(sent_output)[0]\n",
    "\n",
    "        # Un-sort by length\n",
    "        idx_unsort = torch.from_numpy(idx_unsort).cuda() if self.is_cuda() \\\n",
    "            else torch.from_numpy(idx_unsort)\n",
    "        sent_output = sent_output.index_select(1, idx_unsort)\n",
    "\n",
    "        # Pooling\n",
    "        if self.pool_type == \"mean\":\n",
    "            sent_len = torch.FloatTensor(sent_len.copy()).unsqueeze(1).cuda()\n",
    "            emb = torch.sum(sent_output, 0).squeeze(0)\n",
    "            emb = emb / sent_len.expand_as(emb)\n",
    "        elif self.pool_type == \"max\":\n",
    "            if not self.max_pad:\n",
    "                sent_output[sent_output == 0] = -1e9\n",
    "            emb = torch.max(sent_output, 0)[0]\n",
    "            if emb.ndimension() == 3:\n",
    "                emb = emb.squeeze(0)\n",
    "                assert emb.ndimension() == 2\n",
    "\n",
    "        return emb\n",
    "\n",
    "    def set_w2v_path(self, w2v_path):\n",
    "        self.w2v_path = w2v_path\n",
    "\n",
    "    def get_word_dict(self, sentences, tokenize=True):\n",
    "        # create vocab of words\n",
    "        word_dict = {}\n",
    "        sentences = [s.split() if not tokenize else self.tokenize(s) for s in sentences]\n",
    "        for sent in sentences:\n",
    "            for word in sent:\n",
    "                if word not in word_dict:\n",
    "                    word_dict[word] = ''\n",
    "        word_dict[self.bos] = ''\n",
    "        word_dict[self.eos] = ''\n",
    "        return word_dict\n",
    "\n",
    "    def get_w2v(self, word_dict):\n",
    "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
    "        # create word_vec with w2v vectors\n",
    "        word_vec = {}\n",
    "        with open(self.w2v_path, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                word, vec = line.split(' ', 1)\n",
    "                if word in word_dict:\n",
    "                    word_vec[word] = np.fromstring(vec, sep=' ')\n",
    "        print('Found %s(/%s) words with w2v vectors' % (len(word_vec), len(word_dict)))\n",
    "        return word_vec\n",
    "\n",
    "    def get_w2v_k(self, K):\n",
    "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
    "        # create word_vec with k first w2v vectors\n",
    "        k = 0\n",
    "        word_vec = {}\n",
    "        with open(self.w2v_path, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                word, vec = line.split(' ', 1)\n",
    "                if k <= K:\n",
    "                    word_vec[word] = np.fromstring(vec, sep=' ')\n",
    "                    k += 1\n",
    "                if k > K:\n",
    "                    if word in [self.bos, self.eos]:\n",
    "                        word_vec[word] = np.fromstring(vec, sep=' ')\n",
    "\n",
    "                if k > K and all([w in word_vec for w in [self.bos, self.eos]]):\n",
    "                    break\n",
    "        return word_vec\n",
    "\n",
    "    def build_vocab(self, sentences, tokenize=True):\n",
    "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
    "        word_dict = self.get_word_dict(sentences, tokenize)\n",
    "        self.word_vec = self.get_w2v(word_dict)\n",
    "        print('Vocab size : %s' % (len(self.word_vec)))\n",
    "\n",
    "    # build w2v vocab with k most frequent words\n",
    "    def build_vocab_k_words(self, K):\n",
    "        assert hasattr(self, 'w2v_path'), 'w2v path not set'\n",
    "        self.word_vec = self.get_w2v_k(K)\n",
    "        print('Vocab size : %s' % (K))\n",
    "\n",
    "    def update_vocab(self, sentences, tokenize=True):\n",
    "        assert hasattr(self, 'w2v_path'), 'warning : w2v path not set'\n",
    "        assert hasattr(self, 'word_vec'), 'build_vocab before updating it'\n",
    "        word_dict = self.get_word_dict(sentences, tokenize)\n",
    "\n",
    "        # keep only new words\n",
    "        for word in self.word_vec:\n",
    "            if word in word_dict:\n",
    "                del word_dict[word]\n",
    "\n",
    "        # udpate vocabulary\n",
    "        if word_dict:\n",
    "            new_word_vec = self.get_w2v(word_dict)\n",
    "            self.word_vec.update(new_word_vec)\n",
    "        else:\n",
    "            new_word_vec = []\n",
    "        print('New vocab size : %s (added %s words)'% (len(self.word_vec), len(new_word_vec)))\n",
    "\n",
    "    def get_batch(self, batch):\n",
    "        # sent in batch in decreasing order of lengths\n",
    "        # batch: (bsize, max_len, word_dim)\n",
    "        embed = np.zeros((len(batch[0]), len(batch), self.word_emb_dim))\n",
    "\n",
    "        for i in range(len(batch)):\n",
    "            for j in range(len(batch[i])):\n",
    "                embed[j, i, :] = self.word_vec[batch[i][j]]\n",
    "\n",
    "        return torch.FloatTensor(embed)\n",
    "\n",
    "    def tokenize(self, s):\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        if self.moses_tok:\n",
    "            s = ' '.join(word_tokenize(s))\n",
    "            s = s.replace(\" n't \", \"n 't \")  # HACK to get ~MOSES tokenization\n",
    "            return s.split()\n",
    "        else:\n",
    "            return word_tokenize(s)\n",
    "\n",
    "    def prepare_samples(self, sentences, bsize, tokenize, verbose):\n",
    "        sentences = [[self.bos] + s.split() + [self.eos] if not tokenize else\n",
    "                     [self.bos] + self.tokenize(s) + [self.eos] for s in sentences]\n",
    "        n_w = np.sum([len(x) for x in sentences])\n",
    "\n",
    "        # filters words without w2v vectors\n",
    "        for i in range(len(sentences)):\n",
    "            s_f = [word for word in sentences[i] if word in self.word_vec]\n",
    "            if not s_f:\n",
    "                import warnings\n",
    "                warnings.warn('No words in \"%s\" (idx=%s) have w2v vectors. \\\n",
    "                               Replacing by \"</s>\"..' % (sentences[i], i))\n",
    "                s_f = [self.eos]\n",
    "            sentences[i] = s_f\n",
    "\n",
    "        lengths = np.array([len(s) for s in sentences])\n",
    "        n_wk = np.sum(lengths)\n",
    "        if verbose:\n",
    "            print('Nb words kept : %s/%s (%.1f%s)' % (\n",
    "                        n_wk, n_w, 100.0 * n_wk / n_w, '%'))\n",
    "\n",
    "        # sort by decreasing length\n",
    "        lengths, idx_sort = np.sort(lengths)[::-1], np.argsort(-lengths)\n",
    "        sentences = np.array(sentences)[idx_sort]\n",
    "\n",
    "        return sentences, lengths, idx_sort\n",
    "\n",
    "    def encode(self, sentences, bsize=64, tokenize=True, verbose=False):\n",
    "        tic = time.time()\n",
    "        sentences, lengths, idx_sort = self.prepare_samples(\n",
    "                        sentences, bsize, tokenize, verbose)\n",
    "\n",
    "        embeddings = []\n",
    "        for stidx in range(0, len(sentences), bsize):\n",
    "            batch = self.get_batch(sentences[stidx:stidx + bsize])\n",
    "            if self.is_cuda():\n",
    "                batch = batch.cuda()\n",
    "            with torch.no_grad():\n",
    "                batch = self.forward((batch, lengths[stidx:stidx + bsize])).data.cpu().numpy()\n",
    "            embeddings.append(batch)\n",
    "        embeddings = np.vstack(embeddings)\n",
    "\n",
    "        # unsort\n",
    "        idx_unsort = np.argsort(idx_sort)\n",
    "        embeddings = embeddings[idx_unsort]\n",
    "\n",
    "        if verbose:\n",
    "            print('Speed : %.1f sentences/s (%s mode, bsize=%s)' % (\n",
    "                    len(embeddings)/(time.time()-tic),\n",
    "                    'gpu' if self.is_cuda() else 'cpu', bsize))\n",
    "        return embeddings\n",
    "\n",
    "    def visualize(self, sent, tokenize=True):\n",
    "\n",
    "        sent = sent.split() if not tokenize else self.tokenize(sent)\n",
    "        sent = [[self.bos] + [word for word in sent if word in self.word_vec] + [self.eos]]\n",
    "\n",
    "        if ' '.join(sent[0]) == '%s %s' % (self.bos, self.eos):\n",
    "            import warnings\n",
    "            warnings.warn('No words in \"%s\" have w2v vectors. Replacing \\\n",
    "                           by \"%s %s\"..' % (sent, self.bos, self.eos))\n",
    "        batch = self.get_batch(sent)\n",
    "\n",
    "        if self.is_cuda():\n",
    "            batch = batch.cuda()\n",
    "        output = self.enc_lstm(batch)[0]\n",
    "        output, idxs = torch.max(output, 0)\n",
    "        # output, idxs = output.squeeze(), idxs.squeeze()\n",
    "        idxs = idxs.data.cpu().numpy()\n",
    "        argmaxs = [np.sum((idxs == k)) for k in range(len(sent[0]))]\n",
    "\n",
    "        # visualize model\n",
    "        import matplotlib.pyplot as plt\n",
    "        x = range(len(sent[0]))\n",
    "        y = [100.0 * n / np.sum(argmaxs) for n in argmaxs]\n",
    "        plt.xticks(x, sent[0], rotation=45)\n",
    "        plt.bar(x, y)\n",
    "        plt.ylabel('%')\n",
    "        plt.title('Visualisation of words importance')\n",
    "        plt.show()\n",
    "\n",
    "        return output, idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = 1\n",
    "MODEL_PATH = 'infersent%s.pkl' % V\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\n",
    "infersent = InferSent(params_model)\n",
    "infersent.load_state_dict(torch.load(MODEL_PATH))\n",
    "W2V_PATH = 'glove.840B.300d.txt'\n",
    "infersent.set_w2v_path(W2V_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "884"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_json('train-v1.1.json')\n",
    "train.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = []\n",
    "questions = []\n",
    "answers = []\n",
    "titles = []\n",
    "\n",
    "def get_attributes(item):\n",
    "    data = item['data']\n",
    "    title = data['title']\n",
    "    for paragraph in data['paragraphs']:\n",
    "        for qas in paragraph['qas']:\n",
    "            answers.append(qas['answers'][0]['text'])\n",
    "            questions.append(qas['question'])\n",
    "            contexts.append(paragraph['context'])\n",
    "            titles.append(title)\n",
    "            \n",
    "def build_dataframe(train):\n",
    "    train.apply(get_attributes, axis = 1)\n",
    "    train_df = pd.DataFrame({\n",
    "    'contexts':contexts,\n",
    "    'questions': questions,\n",
    "    'answers': answers,\n",
    "    'titles': titles\n",
    "})\n",
    "    return train_df\n",
    "    \n",
    "train_df = build_dataframe(train)\n",
    "train_df = train_df.head(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['sentences'] = train_df['contexts'].apply(lambda x : [item.raw for item in TextBlob(x).sentences ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(item):\n",
    "    \"\"\" Builds the target using the index number of answer in the list of sentences\n",
    "    \"\"\"\n",
    "    for index, sentence in enumerate( item['sentences']):\n",
    "        if item['answers'] in sentence:\n",
    "            return index\n",
    "    return 0\n",
    "    \n",
    "train_df['target'] = train_df.apply(get_target, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(sentences):\n",
    "    all_sentences = []\n",
    "    sentences = sentences.tolist()\n",
    "    for context_sentences in sentences:\n",
    "        for setence in context_sentences:\n",
    "            all_sentences.append(setence)\n",
    "        \n",
    "    all_sentences = list(dict.fromkeys(all_sentences))\n",
    "    return all_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14620(/15387) words with w2v vectors\n",
      "Vocab size : 14620\n"
     ]
    }
   ],
   "source": [
    "paras = list(train_df[\"contexts\"].drop_duplicates().reset_index(drop= True))\n",
    "blob = TextBlob(\" \".join(paras))\n",
    "sentences = get_all_sentences(train_df['sentences'])\n",
    "infersent.build_vocab(sentences, tokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sentence Embeddings\n",
    "dict_embeddings = {}\n",
    "for i in range(len(sentences)):\n",
    "    dict_embeddings[sentences[i]] = infersent.encode([sentences[i]], tokenize=True)[0]\n",
    "  \n",
    "# Question Embeddings\n",
    "questions = list(train_df[\"questions\"])    \n",
    "for i in range(len(questions)):\n",
    "    dict_embeddings[questions[i]] = infersent.encode([questions[i]], tokenize=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_embeddings(item):\n",
    "    embeddings = []\n",
    "    for sentence in item.sentences:\n",
    "        embeddings.append(dict_embeddings[sentence])\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['question_embedding'] = train_df['questions'].apply(lambda x : dict_embeddings[x])\n",
    "train_df['context_embedding'] = train_df.apply(get_context_embeddings, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contexts</th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "      <th>titles</th>\n",
       "      <th>sentences</th>\n",
       "      <th>target</th>\n",
       "      <th>question_embedding</th>\n",
       "      <th>context_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>[Architecturally, the school has a Catholic ch...</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.1101008, 0.1142294, 0.11560898, 0.05489475,...</td>\n",
       "      <td>[[0.055199966, 0.05013141, 0.047870383, 0.0162...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>[Architecturally, the school has a Catholic ch...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.10951651, 0.11030624, 0.052100062, 0.030539...</td>\n",
       "      <td>[[0.055199966, 0.05013141, 0.047870383, 0.0162...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>the Main Building</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>[Architecturally, the school has a Catholic ch...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.011956469, 0.14930709, 0.028481217, 0.05278...</td>\n",
       "      <td>[[0.055199966, 0.05013141, 0.047870383, 0.0162...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>[Architecturally, the school has a Catholic ch...</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0711433, 0.054118324, -0.013959841, 0.05310...</td>\n",
       "      <td>[[0.055199966, 0.05013141, 0.047870383, 0.0162...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>[Architecturally, the school has a Catholic ch...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.16131133, 0.15654244, 0.08214858, 0.0437286...</td>\n",
       "      <td>[[0.055199966, 0.05013141, 0.047870383, 0.0162...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            contexts  \\\n",
       "0  Architecturally, the school has a Catholic cha...   \n",
       "1  Architecturally, the school has a Catholic cha...   \n",
       "2  Architecturally, the school has a Catholic cha...   \n",
       "3  Architecturally, the school has a Catholic cha...   \n",
       "4  Architecturally, the school has a Catholic cha...   \n",
       "\n",
       "                                           questions  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...   \n",
       "1  What is in front of the Notre Dame Main Building?   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...   \n",
       "3                  What is the Grotto at Notre Dame?   \n",
       "4  What sits on top of the Main Building at Notre...   \n",
       "\n",
       "                                   answers                    titles  \\\n",
       "0               Saint Bernadette Soubirous  University_of_Notre_Dame   \n",
       "1                a copper statue of Christ  University_of_Notre_Dame   \n",
       "2                        the Main Building  University_of_Notre_Dame   \n",
       "3  a Marian place of prayer and reflection  University_of_Notre_Dame   \n",
       "4       a golden statue of the Virgin Mary  University_of_Notre_Dame   \n",
       "\n",
       "                                           sentences  target  \\\n",
       "0  [Architecturally, the school has a Catholic ch...       5   \n",
       "1  [Architecturally, the school has a Catholic ch...       2   \n",
       "2  [Architecturally, the school has a Catholic ch...       1   \n",
       "3  [Architecturally, the school has a Catholic ch...       4   \n",
       "4  [Architecturally, the school has a Catholic ch...       1   \n",
       "\n",
       "                                  question_embedding  \\\n",
       "0  [0.1101008, 0.1142294, 0.11560898, 0.05489475,...   \n",
       "1  [0.10951651, 0.11030624, 0.052100062, 0.030539...   \n",
       "2  [0.011956469, 0.14930709, 0.028481217, 0.05278...   \n",
       "3  [0.0711433, 0.054118324, -0.013959841, 0.05310...   \n",
       "4  [0.16131133, 0.15654244, 0.08214858, 0.0437286...   \n",
       "\n",
       "                                   context_embedding  \n",
       "0  [[0.055199966, 0.05013141, 0.047870383, 0.0162...  \n",
       "1  [[0.055199966, 0.05013141, 0.047870383, 0.0162...  \n",
       "2  [[0.055199966, 0.05013141, 0.047870383, 0.0162...  \n",
       "3  [[0.055199966, 0.05013141, 0.047870383, 0.0162...  \n",
       "4  [[0.055199966, 0.05013141, 0.047870383, 0.0162...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric(item, metric):\n",
    "    result = []\n",
    "    for i in range(0,len(item.sentences)):\n",
    "        question_embedding = [item.question_embedding]\n",
    "        sentence_embedding = [item['context_embedding'][i]]\n",
    "\n",
    "        if metric == 'cosine_similarity':\n",
    "            metric = cosine_similarity(question_embedding, sentence_embedding)\n",
    "            \n",
    "        if metric == 'euclidean':\n",
    "            metric = euclidean_distances(question_embedding, sentence_embedding)  \n",
    "\n",
    "        result.append(metric[0][0])  \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['cosine_similarity'] = train_df.apply(lambda item : get_metric(item, 'cosine_similarity'), axis = 1)\n",
    "train_df['euclidean'] = train_df.apply(lambda item : get_metric(item, 'euclidean'), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contexts</th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "      <th>titles</th>\n",
       "      <th>sentences</th>\n",
       "      <th>target</th>\n",
       "      <th>question_embedding</th>\n",
       "      <th>context_embedding</th>\n",
       "      <th>cosine_similarity</th>\n",
       "      <th>euclidean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>[Architecturally, the school has a Catholic ch...</td>\n",
       "      <td>5</td>\n",
       "      <td>[0.1101008, 0.1142294, 0.11560898, 0.05489475,...</td>\n",
       "      <td>[[0.055199966, 0.05013141, 0.047870383, 0.0162...</td>\n",
       "      <td>[0.5752636, 0.5752636, 0.5752636, 0.5752636, 0...</td>\n",
       "      <td>[3.8162625, 3.8162625, 3.8162625, 3.8162625, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>[Architecturally, the school has a Catholic ch...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.10951651, 0.11030624, 0.052100062, 0.030539...</td>\n",
       "      <td>[[0.055199966, 0.05013141, 0.047870383, 0.0162...</td>\n",
       "      <td>[0.5459254, 0.5459254, 0.5459254, 0.5459254, 0...</td>\n",
       "      <td>[3.590196, 3.590196, 3.590196, 3.590196, 3.590...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>the Main Building</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>[Architecturally, the school has a Catholic ch...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.011956469, 0.14930709, 0.028481217, 0.05278...</td>\n",
       "      <td>[[0.055199966, 0.05013141, 0.047870383, 0.0162...</td>\n",
       "      <td>[0.60825235, 0.60825235, 0.60825235, 0.6082523...</td>\n",
       "      <td>[3.4122276, 3.4122276, 3.4122276, 3.4122276, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>[Architecturally, the school has a Catholic ch...</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0711433, 0.054118324, -0.013959841, 0.05310...</td>\n",
       "      <td>[[0.055199966, 0.05013141, 0.047870383, 0.0162...</td>\n",
       "      <td>[0.50993013, 0.50993013, 0.50993013, 0.5099301...</td>\n",
       "      <td>[3.6493201, 3.6493201, 3.6493201, 3.6493201, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>[Architecturally, the school has a Catholic ch...</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.16131133, 0.15654244, 0.08214858, 0.0437286...</td>\n",
       "      <td>[[0.055199966, 0.05013141, 0.047870383, 0.0162...</td>\n",
       "      <td>[0.52223635, 0.52223635, 0.52223635, 0.5222363...</td>\n",
       "      <td>[3.7629066, 3.7629066, 3.7629066, 3.7629066, 3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            contexts  \\\n",
       "0  Architecturally, the school has a Catholic cha...   \n",
       "1  Architecturally, the school has a Catholic cha...   \n",
       "2  Architecturally, the school has a Catholic cha...   \n",
       "3  Architecturally, the school has a Catholic cha...   \n",
       "4  Architecturally, the school has a Catholic cha...   \n",
       "\n",
       "                                           questions  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...   \n",
       "1  What is in front of the Notre Dame Main Building?   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...   \n",
       "3                  What is the Grotto at Notre Dame?   \n",
       "4  What sits on top of the Main Building at Notre...   \n",
       "\n",
       "                                   answers                    titles  \\\n",
       "0               Saint Bernadette Soubirous  University_of_Notre_Dame   \n",
       "1                a copper statue of Christ  University_of_Notre_Dame   \n",
       "2                        the Main Building  University_of_Notre_Dame   \n",
       "3  a Marian place of prayer and reflection  University_of_Notre_Dame   \n",
       "4       a golden statue of the Virgin Mary  University_of_Notre_Dame   \n",
       "\n",
       "                                           sentences  target  \\\n",
       "0  [Architecturally, the school has a Catholic ch...       5   \n",
       "1  [Architecturally, the school has a Catholic ch...       2   \n",
       "2  [Architecturally, the school has a Catholic ch...       1   \n",
       "3  [Architecturally, the school has a Catholic ch...       4   \n",
       "4  [Architecturally, the school has a Catholic ch...       1   \n",
       "\n",
       "                                  question_embedding  \\\n",
       "0  [0.1101008, 0.1142294, 0.11560898, 0.05489475,...   \n",
       "1  [0.10951651, 0.11030624, 0.052100062, 0.030539...   \n",
       "2  [0.011956469, 0.14930709, 0.028481217, 0.05278...   \n",
       "3  [0.0711433, 0.054118324, -0.013959841, 0.05310...   \n",
       "4  [0.16131133, 0.15654244, 0.08214858, 0.0437286...   \n",
       "\n",
       "                                   context_embedding  \\\n",
       "0  [[0.055199966, 0.05013141, 0.047870383, 0.0162...   \n",
       "1  [[0.055199966, 0.05013141, 0.047870383, 0.0162...   \n",
       "2  [[0.055199966, 0.05013141, 0.047870383, 0.0162...   \n",
       "3  [[0.055199966, 0.05013141, 0.047870383, 0.0162...   \n",
       "4  [[0.055199966, 0.05013141, 0.047870383, 0.0162...   \n",
       "\n",
       "                                   cosine_similarity  \\\n",
       "0  [0.5752636, 0.5752636, 0.5752636, 0.5752636, 0...   \n",
       "1  [0.5459254, 0.5459254, 0.5459254, 0.5459254, 0...   \n",
       "2  [0.60825235, 0.60825235, 0.60825235, 0.6082523...   \n",
       "3  [0.50993013, 0.50993013, 0.50993013, 0.5099301...   \n",
       "4  [0.52223635, 0.52223635, 0.52223635, 0.5222363...   \n",
       "\n",
       "                                           euclidean  \n",
       "0  [3.8162625, 3.8162625, 3.8162625, 3.8162625, 3...  \n",
       "1  [3.590196, 3.590196, 3.590196, 3.590196, 3.590...  \n",
       "2  [3.4122276, 3.4122276, 3.4122276, 3.4122276, 3...  \n",
       "3  [3.6493201, 3.6493201, 3.6493201, 3.6493201, 3...  \n",
       "4  [3.7629066, 3.7629066, 3.7629066, 3.7629066, 3...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_copy = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_number_of_sentences():\n",
    "    \"\"\"\n",
    "        finds the maximum number of sentences possible in any context\n",
    "    \"\"\"\n",
    "    max_number_of_sentences = 0\n",
    "    for i in range(0, train_df.shape[0]):\n",
    "        length = len(train_df.iloc[i].sentences)\n",
    "        if length > max_number_of_sentences:\n",
    "            max_number_of_sentences = length  \n",
    "    return max_number_of_sentences     \n",
    "    \n",
    "max_number_of_sentences = find_max_number_of_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_number_of_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(data, max_length):\n",
    "    mean = sum(data)/len(data)\n",
    "    length_of_data = len(data)\n",
    "    pad_number = max_length - length_of_data\n",
    "    data = data + [mean]*pad_number\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultant_data = []\n",
    "def combine_features(item):\n",
    "    \"\"\"\n",
    "    Pads the euclidean and cosine values for particualr instance and generates resultant dataframe\n",
    "    for modelling , it has eculidean distance between question and all sentnces and cosine similarity\n",
    "    between between question and all sentences as well and last feature is the index of the answer in the sentnces\n",
    "    \"\"\"\n",
    "    length_of_sentence = len(item.sentences)\n",
    "    cosine_similarity = item.cosine_similarity\n",
    "    euclidean = item.euclidean\n",
    "    \n",
    "    if length_of_sentence < max_number_of_sentences:\n",
    "        euclidean = pad(euclidean, max_number_of_sentences)\n",
    "        cosine_similarity = pad(cosine_similarity, max_number_of_sentences)\n",
    "        \n",
    "    features = euclidean + cosine_similarity + [item.target]    \n",
    "    resultant_data.append(features)\n",
    "train_df_copy.apply(combine_features, axis = 1)\n",
    "\n",
    "resultant_data = pd.DataFrame(resultant_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.816262</td>\n",
       "      <td>3.816262</td>\n",
       "      <td>3.816262</td>\n",
       "      <td>3.816262</td>\n",
       "      <td>3.816262</td>\n",
       "      <td>3.816262</td>\n",
       "      <td>3.816262</td>\n",
       "      <td>3.816262</td>\n",
       "      <td>3.816262</td>\n",
       "      <td>3.816262</td>\n",
       "      <td>...</td>\n",
       "      <td>0.575264</td>\n",
       "      <td>0.575264</td>\n",
       "      <td>0.575264</td>\n",
       "      <td>0.575264</td>\n",
       "      <td>0.575264</td>\n",
       "      <td>0.575264</td>\n",
       "      <td>0.575264</td>\n",
       "      <td>0.575264</td>\n",
       "      <td>0.575264</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.590196</td>\n",
       "      <td>3.590196</td>\n",
       "      <td>3.590196</td>\n",
       "      <td>3.590196</td>\n",
       "      <td>3.590196</td>\n",
       "      <td>3.590196</td>\n",
       "      <td>3.590196</td>\n",
       "      <td>3.590196</td>\n",
       "      <td>3.590196</td>\n",
       "      <td>3.590196</td>\n",
       "      <td>...</td>\n",
       "      <td>0.545925</td>\n",
       "      <td>0.545925</td>\n",
       "      <td>0.545925</td>\n",
       "      <td>0.545925</td>\n",
       "      <td>0.545925</td>\n",
       "      <td>0.545925</td>\n",
       "      <td>0.545925</td>\n",
       "      <td>0.545925</td>\n",
       "      <td>0.545925</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.412228</td>\n",
       "      <td>3.412228</td>\n",
       "      <td>3.412228</td>\n",
       "      <td>3.412228</td>\n",
       "      <td>3.412228</td>\n",
       "      <td>3.412228</td>\n",
       "      <td>3.412228</td>\n",
       "      <td>3.412228</td>\n",
       "      <td>3.412228</td>\n",
       "      <td>3.412228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.608252</td>\n",
       "      <td>0.608252</td>\n",
       "      <td>0.608252</td>\n",
       "      <td>0.608252</td>\n",
       "      <td>0.608252</td>\n",
       "      <td>0.608252</td>\n",
       "      <td>0.608252</td>\n",
       "      <td>0.608252</td>\n",
       "      <td>0.608252</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.649320</td>\n",
       "      <td>3.649320</td>\n",
       "      <td>3.649320</td>\n",
       "      <td>3.649320</td>\n",
       "      <td>3.649320</td>\n",
       "      <td>3.649320</td>\n",
       "      <td>3.649320</td>\n",
       "      <td>3.649320</td>\n",
       "      <td>3.649320</td>\n",
       "      <td>3.649320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.509930</td>\n",
       "      <td>0.509930</td>\n",
       "      <td>0.509930</td>\n",
       "      <td>0.509930</td>\n",
       "      <td>0.509930</td>\n",
       "      <td>0.509930</td>\n",
       "      <td>0.509930</td>\n",
       "      <td>0.509930</td>\n",
       "      <td>0.509930</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.762907</td>\n",
       "      <td>3.762907</td>\n",
       "      <td>3.762907</td>\n",
       "      <td>3.762907</td>\n",
       "      <td>3.762907</td>\n",
       "      <td>3.762907</td>\n",
       "      <td>3.762907</td>\n",
       "      <td>3.762907</td>\n",
       "      <td>3.762907</td>\n",
       "      <td>3.762907</td>\n",
       "      <td>...</td>\n",
       "      <td>0.522236</td>\n",
       "      <td>0.522236</td>\n",
       "      <td>0.522236</td>\n",
       "      <td>0.522236</td>\n",
       "      <td>0.522236</td>\n",
       "      <td>0.522236</td>\n",
       "      <td>0.522236</td>\n",
       "      <td>0.522236</td>\n",
       "      <td>0.522236</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  3.816262  3.816262  3.816262  3.816262  3.816262  3.816262  3.816262   \n",
       "1  3.590196  3.590196  3.590196  3.590196  3.590196  3.590196  3.590196   \n",
       "2  3.412228  3.412228  3.412228  3.412228  3.412228  3.412228  3.412228   \n",
       "3  3.649320  3.649320  3.649320  3.649320  3.649320  3.649320  3.649320   \n",
       "4  3.762907  3.762907  3.762907  3.762907  3.762907  3.762907  3.762907   \n",
       "\n",
       "         7         8         9   ...        35        36        37        38  \\\n",
       "0  3.816262  3.816262  3.816262  ...  0.575264  0.575264  0.575264  0.575264   \n",
       "1  3.590196  3.590196  3.590196  ...  0.545925  0.545925  0.545925  0.545925   \n",
       "2  3.412228  3.412228  3.412228  ...  0.608252  0.608252  0.608252  0.608252   \n",
       "3  3.649320  3.649320  3.649320  ...  0.509930  0.509930  0.509930  0.509930   \n",
       "4  3.762907  3.762907  3.762907  ...  0.522236  0.522236  0.522236  0.522236   \n",
       "\n",
       "         39        40        41        42        43  44  \n",
       "0  0.575264  0.575264  0.575264  0.575264  0.575264   5  \n",
       "1  0.545925  0.545925  0.545925  0.545925  0.545925   2  \n",
       "2  0.608252  0.608252  0.608252  0.608252  0.608252   1  \n",
       "3  0.509930  0.509930  0.509930  0.509930  0.509930   4  \n",
       "4  0.522236  0.522236  0.522236  0.522236  0.522236   1  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultant_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = resultant_data.iloc[:,:-1]\n",
    "y = resultant_data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(X, y, train_size=0.8, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Logistic regression Train Accuracy :  0.374\n",
      "Multinomial Logistic regression Test Accuracy :  0.384\n"
     ]
    }
   ],
   "source": [
    "mul_lr = linear_model.LogisticRegression(multi_class='multinomial', solver='newton-cg')\n",
    "mul_lr.fit(train_x, train_y)\n",
    "\n",
    "print(\"Multinomial Logistic regression Train Accuracy : \", metrics.accuracy_score(train_y, mul_lr.predict(train_x)))\n",
    "print(\"Multinomial Logistic regression Test Accuracy : \", metrics.accuracy_score(test_y, mul_lr.predict(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Logistic regression Train Accuracy :  0.54975\n",
      "Multinomial Logistic regression Test Accuracy :  0.359\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(min_samples_leaf=8, n_estimators=60)\n",
    "rf.fit(train_x, train_y)\n",
    "\n",
    "print(\"Multinomial Logistic regression Train Accuracy : \", metrics.accuracy_score(train_y, rf.predict(train_x)))\n",
    "print(\"Multinomial Logistic regression Test Accuracy : \", metrics.accuracy_score(test_y, rf.predict(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
